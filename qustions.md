# 思考题

## 第一阶段
 
> 1、本实验的数据量较大，如何通过最简单的编程使用最快的速度和效率获取所需要的数据？

1. 并行处理：使用多线程或多进程并行处理数据的获取。这可以加速数据的抓取和处理，特别是当数据源是分布式的时候。
2. 数据流式处理：不必一次性获取所有数据，而是采用流式处理的方式逐步获取数据，从而减少内存占用和提高效率。
3. 数据缓存：将已获取的数据缓存在本地，避免重复请求相同数据，节省带宽和时间。

> 2、思考在数据读取和写出部分的文字编码问题。若存在乱码问题，则采用何种技术给予解决？

1. 字符编码设置：确保在读取和写出数据时，使用正确的字符编码方式。通常，UTF-8是一个通用的字符编码，可用于多语言文本。
2. 自动检测编码：使用自动编码检测库，如chardet（Python）或iconv（Linux命令行工具），来检测文本的字符编码，然后进行相应的解码和编码。
3. 数据清洗：在数据预处理阶段，可以识别和处理包含乱码的文本，或者将其删除或替换为合适的字符。
4. 数据标记：在数据中标记文本的编码方式，以便后续处理时能够正确解码。

> 3、横向对比不同分词方法的特点和性能，以及思考如何提高分词方法的有效性？

1. 不同分词方法性能
- 基于规则的分词：使用事先定义的规则和词典来划分文本。适用于特定领域或语言。
- 基于统计的分词：利用统计模型和语料库来识别文本中的词语。如中文分词中的HMM（隐马尔可夫模型）或CRF（条件随机场）方法。
- 基于深度学习的分词：使用深度学习模型，如循环神经网络（RNN）或Transformer模型，来进行分词。例如，Bert分词模型。
2. 提高分词方法有效性
- 使用更大的语料库：增加语料库的规模可以提高统计方法的准确性。
- 针对特定领域进行定制：根据应用领域的特点，定制词典和规则，以提高分词效果。
- 结合多种方法：可以结合多个分词方法，如规则和统计方法，以获得更好的效果。

## 第二阶段

> 1.试说明整个算法的流程

1. 预处理：使用`jieba`库对原始数据进行分词，同时去除停用词和关键词本身
2. 计算中介关键词：使用`TfidfVectorizer`类计算数据的`TF-IDF`矩阵，计算出中介关键词及其`TF-IDF`分数
3. 计算竞争性关键词：根据`TF-IDF`分数字典，根据分数对关键词进行降序排序

TF-IDF：
1. 分词
2. 计算词频（TF）：词频按0/1表示（归一化处理，出现/不出现）
3. 计算逆文档频率（IDF）：将数据集中的数据行数除以包含该词的行数，然后取对数
4. 计算`TF-IDF`：`TF`分数与`IDF`分数的乘积。分数越高，词重要性越大
5. 构建`TF-IDF`矩阵：对于数据集的每一行，构建一个矩阵。行表示原始数据行，列表示词汇表中的词，矩阵元素对应分数
6. 过滤`TF-IDF`：其中过滤了出现太频繁/不够频繁的词（例如分数为1的词）

> 2.说明程序设计的合理性，分析当前的设计缺陷，以及进一步优化的方法

合理性：
- 停用词过滤：过滤无意义的词，提升分析准确性
- `TF-IDF`计算：利用`TfidfVectorizer`的高效实现，对数据集进行向量化处理，计算每个词的`TF-IDF分数`

设计缺陷：
- 硬编码参数：程序中部分参数（如`min_df`，`max_df`）被硬编码在函数中，降低了程序的灵活性
- 性能问题：使用`jieba`进行中文分词可能遇到性能瓶颈，未考虑分词效率优化
- 并行处理：暂未采用并行加速计算
- 过滤`TF-IDF`分数：程序在写文件时过滤了分数1.0的词，但如果这些词实际上非常重要，可能丢失信息

优化方法：
- 配置文件：通过创建配置文件管理原先硬编码的参数
- 分词效率：对`jieba`分词器进行性能优化，例如使用并行分词（`posix`系统）或采用更高效的分词工具
- 并行计算：使用并行计算库（如`joblib`）来并行化`TF-IDF`计算
- 智能过滤：对于`TF-IDF`分数的过滤设计更智能的策略，如考虑分数分布等，来确定哪些词应该被保留
- 内存管理：对于超大数据集考虑内存使用情况，考虑使用外存处理或数据流式处理